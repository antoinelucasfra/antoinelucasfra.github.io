[
  {
    "objectID": "TODO.html",
    "href": "TODO.html",
    "title": "Antoine Lucas",
    "section": "",
    "text": "todo list :\n\ncr√©er brand.yml √† mon image perso\nrecup/tri/agregaer toutes les ressources dans un dossier (r√©cup sources du dossier it&m + pc it&m cf mission sanofi, loreal, msio)\nutiliser rag pour r√©sumer toutes ces ressources et ce faire des cheatsheets sur ces savoirs)\ncr√©er site web perso : cf inspi mcanouil\ntrier lien + sanofi export\ninclude cv, about, projects, blog tabs\nexplain in blog posts the projects (shiny app for meetups, skate deep learning/huggingface)\n\n\nfetch all links from all sources (words docs, google keeps, chrome windows, etc‚Ä¶)\nsort all the links into categories (methods/random forest, IT/linux stuffs, deployment/kubernetes, etc‚Ä¶)\n\n\nget value from the links ‚Äì&gt; explaining what‚Äôs the use and redacting small examples about what can be done\nmake A REAL STATISTICAL COURSES or point out to this + an R for beginners tutorial\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "A selection of data science projects I‚Äôve worked on in the pharma and cosmetic industries.\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nüìö Resources Catalog\n\n\n\nR\n\nPython\n\nResources\n\n\n\nA curated and searchable collection of 200+ data science resources for R, Python, and beyond. Filter by type, language, and category.\n\n\n\n\n\n\n\n\n\n\n\n\nClinical Trial Dashboard\n\n\n\nR\n\nShiny\n\nHealthcare\n\n\n\nInteractive Shiny dashboard for monitoring clinical trial metrics, patient enrollment, and safety signals in real-time.\n\n\n\n\n\n\n\n\n\n\n\n\nFormulation Optimization Tool\n\n\n\nPython\n\nML\n\nCosmetics\n\n\n\nMachine learning model to predict optimal formulation parameters based on ingredient properties and stability requirements.\n\n\n\n\n\n\n\n\n\n\n\n\nQuality Control Analytics\n\n\n\nR\n\nSPC\n\nPharma\n\n\n\nAutomated statistical process control system for manufacturing quality metrics with real-time alerting.\n\n\n\n\n\n\n\n\n\n\n\n\nStability Prediction Model\n\n\n\nPython\n\nML\n\nCosmetics\n\n\n\nPredictive model for accelerated stability testing, reducing time-to-market for new formulations.\n\n\n\n\n\n\n\n\n\n\n\n\nSupply Chain Demand Forecasting\n\n\n\nPython\n\nForecasting\n\nPharma\n\n\n\nTime series forecasting system for pharmaceutical supply chain optimization using Prophet and XGBoost.\n\n\n\n\n\n\n\n\n\n\n\n\nBatch Record Analyzer\n\n\n\nPython\n\nNLP\n\nPharma\n\n\n\nNLP-powered tool for extracting insights from manufacturing batch records and identifying deviation patterns.\n\n\n\n\n\nNo matching items\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/reproducible-pharma-r/index.html",
    "href": "posts/reproducible-pharma-r/index.html",
    "title": "Building Reproducible Analyses in Pharma with R",
    "section": "",
    "text": "In pharmaceutical data science, reproducibility isn‚Äôt just a best practice‚Äîit‚Äôs a regulatory requirement. This post explores how to build reproducible analysis workflows using R, renv, and Quarto."
  },
  {
    "objectID": "posts/reproducible-pharma-r/index.html#introduction",
    "href": "posts/reproducible-pharma-r/index.html#introduction",
    "title": "Building Reproducible Analyses in Pharma with R",
    "section": "",
    "text": "In pharmaceutical data science, reproducibility isn‚Äôt just a best practice‚Äîit‚Äôs a regulatory requirement. This post explores how to build reproducible analysis workflows using R, renv, and Quarto."
  },
  {
    "objectID": "posts/reproducible-pharma-r/index.html#why-reproducibility-matters-in-pharma",
    "href": "posts/reproducible-pharma-r/index.html#why-reproducibility-matters-in-pharma",
    "title": "Building Reproducible Analyses in Pharma with R",
    "section": "Why Reproducibility Matters in Pharma",
    "text": "Why Reproducibility Matters in Pharma\nRegulatory agencies like the FDA and EMA require that analytical work be:\n\n\nDocumented ‚Äî Clear records of what was done\n\nTraceable ‚Äî Ability to follow the analysis from raw data to conclusions\n\nReproducible ‚Äî The same inputs should produce the same outputs"
  },
  {
    "objectID": "posts/reproducible-pharma-r/index.html#key-tools-for-reproducibility",
    "href": "posts/reproducible-pharma-r/index.html#key-tools-for-reproducibility",
    "title": "Building Reproducible Analyses in Pharma with R",
    "section": "Key Tools for Reproducibility",
    "text": "Key Tools for Reproducibility\n1. Version Control with Git\ngit init\ngit add .\ngit commit -m \"Initial analysis setup\"\n2. Dependency Management with renv\n\n# Initialize renv for the project\nrenv::init()\n\n# Snapshot current dependencies\nrenv::snapshot()\n\n# Restore dependencies on a new machine\nrenv::restore()\n\n3. Pipeline Management with targets\n\nlibrary(targets)\n\n# Define your analysis pipeline\nlist(\n  tar_target(raw_data, read_csv(\"data/clinical_data.csv\")),\n  tar_target(clean_data, clean_clinical_data(raw_data)),\n  tar_target(analysis, run_statistical_analysis(clean_data)),\n  tar_target(report, render_report(analysis))\n)"
  },
  {
    "objectID": "posts/reproducible-pharma-r/index.html#best-practices",
    "href": "posts/reproducible-pharma-r/index.html#best-practices",
    "title": "Building Reproducible Analyses in Pharma with R",
    "section": "Best Practices",
    "text": "Best Practices\n\n\nUse project-relative paths ‚Äî Never use absolute paths\n\nDocument your environment ‚Äî Include session info in reports\n\nValidate data inputs ‚Äî Check data quality before analysis\n\nVersion control everything ‚Äî Code, configuration, and documentation"
  },
  {
    "objectID": "posts/reproducible-pharma-r/index.html#conclusion",
    "href": "posts/reproducible-pharma-r/index.html#conclusion",
    "title": "Building Reproducible Analyses in Pharma with R",
    "section": "Conclusion",
    "text": "Conclusion\nBuilding reproducible analyses requires upfront investment but pays dividends in quality, compliance, and peace of mind."
  },
  {
    "objectID": "posts/reproducible-pharma-r/index.html#session-info",
    "href": "posts/reproducible-pharma-r/index.html#session-info",
    "title": "Building Reproducible Analyses in Pharma with R",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()"
  },
  {
    "objectID": "posts/spc-cosmetics-r/index.html",
    "href": "posts/spc-cosmetics-r/index.html",
    "title": "Statistical Process Control for Cosmetic Manufacturing",
    "section": "",
    "text": "Statistical Process Control (SPC) is essential for maintaining consistent product quality in cosmetic manufacturing. This post demonstrates how to implement SPC charts using R."
  },
  {
    "objectID": "posts/spc-cosmetics-r/index.html#introduction",
    "href": "posts/spc-cosmetics-r/index.html#introduction",
    "title": "Statistical Process Control for Cosmetic Manufacturing",
    "section": "",
    "text": "Statistical Process Control (SPC) is essential for maintaining consistent product quality in cosmetic manufacturing. This post demonstrates how to implement SPC charts using R."
  },
  {
    "objectID": "posts/spc-cosmetics-r/index.html#the-basics-of-spc",
    "href": "posts/spc-cosmetics-r/index.html#the-basics-of-spc",
    "title": "Statistical Process Control for Cosmetic Manufacturing",
    "section": "The Basics of SPC",
    "text": "The Basics of SPC\nSPC uses control charts to monitor process behavior over time. Key concepts:\n\n\nCenter Line (CL) ‚Äî The process average\n\nUpper Control Limit (UCL) ‚Äî CL + 3œÉ\n\nLower Control Limit (LCL) ‚Äî CL - 3œÉ"
  },
  {
    "objectID": "posts/spc-cosmetics-r/index.html#implementing-control-charts-in-r",
    "href": "posts/spc-cosmetics-r/index.html#implementing-control-charts-in-r",
    "title": "Statistical Process Control for Cosmetic Manufacturing",
    "section": "Implementing Control Charts in R",
    "text": "Implementing Control Charts in R\nX-bar Chart Example\n\nlibrary(qcc)\n\n# Sample viscosity measurements from production batches\nviscosity &lt;- c(\n  4520, 4535, 4510, 4525, 4540,\n  4515, 4530, 4505, 4545, 4520,\n  4525, 4510, 4535, 4515, 4530\n)\n\n# Create groups (5 samples per subgroup)\ngroups &lt;- rep(1:3, each = 5)\n\n# Create X-bar chart\nqcc(viscosity, type = \"xbar\", sizes = 5)\n\nR Chart for Variability\n\n# R chart for within-subgroup variability\nqcc(viscosity, type = \"R\", sizes = 5)"
  },
  {
    "objectID": "posts/spc-cosmetics-r/index.html#detecting-out-of-control-conditions",
    "href": "posts/spc-cosmetics-r/index.html#detecting-out-of-control-conditions",
    "title": "Statistical Process Control for Cosmetic Manufacturing",
    "section": "Detecting Out-of-Control Conditions",
    "text": "Detecting Out-of-Control Conditions\nCommon patterns to watch for:\n\n\nPoints beyond control limits ‚Äî Immediate investigation needed\n\nRun of 7+ points ‚Äî Process shift detected\n\nTrends ‚Äî Gradual drift in process\n\nHugging the center line ‚Äî Possible measurement issues"
  },
  {
    "objectID": "posts/spc-cosmetics-r/index.html#creating-an-automated-spc-dashboard",
    "href": "posts/spc-cosmetics-r/index.html#creating-an-automated-spc-dashboard",
    "title": "Statistical Process Control for Cosmetic Manufacturing",
    "section": "Creating an Automated SPC Dashboard",
    "text": "Creating an Automated SPC Dashboard\n\nlibrary(shiny)\nlibrary(qcc)\nlibrary(ggplot2)\n\n# Simple Shiny app structure\nui &lt;- fluidPage(\n  titlePanel(\"Production Quality Monitor\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"product\", \"Select Product:\",\n                  choices = c(\"Moisturizer A\", \"Serum B\", \"Cleanser C\"))\n    ),\n    mainPanel(\n      plotOutput(\"controlChart\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$controlChart &lt;- renderPlot({\n    # Generate control chart based on selected product\n    # ... chart generation code\n  })\n}"
  },
  {
    "objectID": "posts/spc-cosmetics-r/index.html#conclusion",
    "href": "posts/spc-cosmetics-r/index.html#conclusion",
    "title": "Statistical Process Control for Cosmetic Manufacturing",
    "section": "Conclusion",
    "text": "Conclusion\nSPC is a powerful tool for maintaining quality in cosmetic manufacturing. With R, you can automate monitoring and quickly detect process issues."
  },
  {
    "objectID": "posts/spc-cosmetics-r/index.html#further-reading",
    "href": "posts/spc-cosmetics-r/index.html#further-reading",
    "title": "Statistical Process Control for Cosmetic Manufacturing",
    "section": "Further Reading",
    "text": "Further Reading\n\nMontgomery, D.C. ‚ÄúIntroduction to Statistical Quality Control‚Äù\nISO 22716: Cosmetics Good Manufacturing Practices"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About This Site",
    "section": "",
    "text": "I‚Äôm Antoine Lucas, a Data Scientist with 5 years of experience in the pharmaceutical and cosmetic industries. This website serves as my professional portfolio and a place to share knowledge with the data science community."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About This Site",
    "section": "",
    "text": "I‚Äôm Antoine Lucas, a Data Scientist with 5 years of experience in the pharmaceutical and cosmetic industries. This website serves as my professional portfolio and a place to share knowledge with the data science community."
  },
  {
    "objectID": "about.html#about-this-website",
    "href": "about.html#about-this-website",
    "title": "About This Site",
    "section": "About This Website",
    "text": "About This Website\nThis site is built with Quarto, an open-source scientific and technical publishing system. It‚Äôs hosted on GitHub Pages and automatically deployed with each commit.\n\nFeatures\n\nüìù Blog ‚Äî Technical articles and tutorials\nüíº Projects ‚Äî Portfolio of data science work\nüìö Resources ‚Äî Curated learning materials"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About This Site",
    "section": "Contact",
    "text": "Contact\nI‚Äôm always interested in discussing data science challenges and opportunities. Feel free to reach out:\n\nEmail: antoinelucas@example.com\nLinkedIn: linkedin.com/in/antoinelucasdata\nGitHub: github.com/antoinelucasfra\nTwitter: @antoineloucass"
  },
  {
    "objectID": "about.html#colophon",
    "href": "about.html#colophon",
    "title": "About This Site",
    "section": "Colophon",
    "text": "Colophon\n\nBuilt with Quarto\nHosted on GitHub Pages\nSource code on GitHub"
  },
  {
    "objectID": "projects/resources_catalog.html",
    "href": "projects/resources_catalog.html",
    "title": "üìö Resources Catalog",
    "section": "",
    "text": "This resource list was last updated on Dec 13, 2025. Use the search box and filters to find resources by type, language, or category.\n\n\n\n\n\n\n\n\n\n\n\n\n\nContributing\n\n\n\nWant to add a resource? Edit the data/resources.csv file and submit a PR on GitHub.\n\n\nLinks to external websites are provided for convenience and informational purposes only.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog where I share insights, tutorials, and thoughts on data science, statistics, and working in the pharma/cosmetic industries.\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nWhen Code Becomes Music: The Art of Algorithmic Live Performance\n\n\n\nCreative Coding\n\nMusic\n\nArt\n\nData\n\n\n\nExploring the fusion of programming, DJing, and data-driven creativity through live coding tools like Strudel‚Äîwhere algorithms meet artistic expression.\n\n\n\n\n\nDecember 13, 2024\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nThe Reproducibility Quest: Modern Tools for R and Python Data Science\n\n\n\nR\n\nPython\n\nReproducibility\n\nDevContainers\n\nDocker\n\nPackage Management\n\nBest Practices\n\n\n\nA comprehensive guide to achieving reproducible data science workflows using modern package managers, containerization, and environment control tools for both R and Python ‚Äî plus how to scope the right level of reproducibility for your needs.\n\n\n\n\n\nDecember 13, 2024\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\nStaying Current in a Fast-Moving Field: Why I Built a Resources Catalog\n\n\n\nData Science\n\nMachine Learning\n\nGenAI\n\nResources\n\nLifelong Learning\n\nBest Practices\n\n\n\nIn data science, machine learning, and AI, the pace of change is relentless. Here‚Äôs why I maintain a curated collection of resources ‚Äî and why you might want to do the same.\n\n\n\n\n\nDecember 13, 2024\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nrv: The Modern R Package Manager for Reproducible Workflows\n\n\n\nR\n\nPackage Management\n\nReproducibility\n\nRust\n\nModern Workflow\n\n\n\nDiscover rv, the new declarative R package manager written in Rust. Learn how it compares to renv, draws inspiration from Python‚Äôs uv, and pairs with rig for a modern R development workflow.\n\n\n\n\n\nDecember 13, 2024\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nBuilding Reproducible Analyses in Pharma with R\n\n\n\nR\n\nPharma\n\nReproducibility\n\nBest Practices\n\n\n\nA practical guide to creating reproducible data analysis workflows in regulated pharmaceutical environments using R and Quarto.\n\n\n\n\n\nNovember 15, 2024\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nComplete Development Setup Guide for R & Python\n\n\n\nR\n\nPython\n\nVS Code\n\nSetup\n\nTutorial\n\n\n\nA comprehensive guide to setting up R, Python, and VS Code for modern data science development workflows.\n\n\n\n\n\nOctober 15, 2024\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nStatistical Process Control for Cosmetic Manufacturing\n\n\n\nR\n\nQuality Control\n\nCosmetics\n\nSPC\n\n\n\nHow to implement SPC charts in R for quality monitoring in cosmetic product manufacturing.\n\n\n\n\n\nSeptember 20, 2024\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nStatistical Theory: Frequentist Methods Reference\n\n\n\nStatistics\n\nTheory\n\nR\n\nReference\n\n\n\nA comprehensive reference guide to frequentist statistical tests, from basic comparisons to regression models.\n\n\n\n\n\nAugust 20, 2024\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with Quarto for Data Science Reports\n\n\n\nQuarto\n\nTutorial\n\nReproducibility\n\n\n\nA beginner‚Äôs guide to creating beautiful, reproducible reports using Quarto in your data science workflow.\n\n\n\n\n\nJuly 10, 2024\n\n2 min\n\n\n\n\nNo matching items\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Looking for data science resources? Check out the üìö Resources Catalog ‚Äî a searchable collection of 200+ curated links for R, Python, and beyond."
  },
  {
    "objectID": "resources.html#quick-links",
    "href": "resources.html#quick-links",
    "title": "Resources",
    "section": "Quick Links",
    "text": "Quick Links\n\nR Programming\n\nTidyverse - Collection of R packages for data science\nR for Data Science - Free online book\nRStudio Cheatsheets - Quick reference guides\n\n\n\nPython Data Science\n\nPython Data Science Handbook - Free online book\nPandas Documentation - Data manipulation library\nScikit-learn - Machine learning in Python\n\n\n\nPharma & Clinical Trials\n\nR/Pharma - R in Pharmaceutical Industry\nCDISC - Clinical Data Interchange Standards\nICH Guidelines - International Council for Harmonisation\n\n\n\nVisualization\n\nggplot2 - Grammar of graphics for R\nPlotly - Interactive visualizations\nShiny - Web applications with R\n\n\nFor a comprehensive searchable catalog of all resources, visit the Resources Catalog."
  },
  {
    "objectID": "posts/stats-theory-frequentist/index.html",
    "href": "posts/stats-theory-frequentist/index.html",
    "title": "Statistical Theory: Frequentist Methods Reference",
    "section": "",
    "text": "This page serves as a reference for common frequentist statistical methods and their implementation in R."
  },
  {
    "objectID": "posts/stats-theory-frequentist/index.html#introduction",
    "href": "posts/stats-theory-frequentist/index.html#introduction",
    "title": "Statistical Theory: Frequentist Methods Reference",
    "section": "",
    "text": "This page serves as a reference for common frequentist statistical methods and their implementation in R."
  },
  {
    "objectID": "posts/stats-theory-frequentist/index.html#theory-on-statistical-tests",
    "href": "posts/stats-theory-frequentist/index.html#theory-on-statistical-tests",
    "title": "Statistical Theory: Frequentist Methods Reference",
    "section": "Theory on Statistical Tests",
    "text": "Theory on Statistical Tests\nLes diff√©rents tests statistiques : P-value : c‚Äôest la proba que le hasard peut expliquer tout seul 1 diff√©rence AU MOINS aussi importante que celle observ√©e. Il y a 2 types d‚Äôhypoth√®ses : ‚Ä¢ Neyman/pearson : l‚Äôhypoth√®se H0 : pa= pb et H1 : pa != pb avec par ex p=0.049 et p=0.00001 ont la MEME ¬´ force ¬ª de test ‚Ä¢ Fischer : pareil mais p=0.049 et p = 0.0000001 ont PAS DU TOUT la m√™me force de test\nCOMPARAISON DE 2 POURCENTAGES\nProp.table(dataframe, ¬Ω) =regarde le % de 1ere variable // √† 2eme (2/1 si inverse) ‚Ä¢ Test du chi-2 : chisq.test(x,y,correct=FALSE/TRUE) AVEC les hypoth√®ses de validit√© (% loin de 0 et 100 et grand √©chantillon) ‚Ä¢ Test fischer si aucune hypoth√®se v√©rifi√©e : fisher.test(x,y)\nCOMPARAISON DE 2 MOYENNES\n‚Ä¢ Test de student : n30 avec une distribution normale. Diagramme de normalit√© : qqnorm() et qqline(x). Calcul variance : by(x,y,sd,na.rm=TRUE) T.test(x~y,var.equal=TRUE) ÔÉ®si pas d‚Äôhypoth√®ses : wilcox.test(x~y) Test nullit√© d‚Äôun coeff de corr√©lation, ‚Ä¢ Test nullit√© de corr√©lation entre 2 variables : 1 des 2 variables doit suivre la loi normale =pearson : cor.test(x,y) ‚Ä¢ Spearman(x,y,method= ¬ªspearman ¬ª) Comparaison de moyenne // √† moyenne de r√©f√©rence ‚Ä¢ T.test(var,varfixe=XXXX) ‚Ä¢ Mcnemar.test(var avant, var apr√®s) POUR VAR QUALI ‚Ä¢ T.test (d√©but,fin,paired=TRUE) POUR VAR QUANTI\nLE T-TEST ANALYSE LES ECARTS TYPES ET LE F-TEST ANALYSE LES MOYENNES\nTEST NON PARAMETRIQUES Test de wilcoxon : comparaison de K= 2 echantillons independants par rapport a une variable X de nature : Quantitative/Qualitative ordinale C‚Äôest le test de l‚Äôhypoth√®se selon laquelle les m√©dianes de chacun de deux groupes de donn√©es sont proche Test de cruskal wallis : est une comparaison de K=3 √©chantillons. Ainsi, l‚Äôinterpr√©tation du test de Kruskal-Wallis est tr√®s similaire √† une ANOVA param√©trique d‚Äôordre Un, sauf qu‚Äôil est bas√© sur les rangs au lieu des moyennes.\nREGRESSION LINEAIRE SIMPLE (GENERALISATION DU TEST DE STUDENT) Mod√®le :lm(x~y,data= dataframe)\nREGRESSION LINEAIRE MULTIPLE :\nLes hypoth√®ses √† v√©rifier sont : normalit√© du bruit et terme r√©siduel, le bruit est ind√©pendant des valeurs des autres variables, ABSENCE de corr√©lation √©vidente, interne avec le bruit Lm(x~y+w+z,data = dataframe) Var quanti = somme (var quanti ou binaire MAIS PAS QUALI) si on veut des quali=codage √† la main Interactions entre 2 var = on met un * Analyse de variance/covariance = cas particulier de r√©gression lin√©aire multiple : toutes les variables explicatives sont cat√©gorielles ou quantitatives\nREGRESSION LOGISTIQUE\nSi la variable √† expliquer est non quantitative : -logistique -transforme les variables Glm(mod√®le, data = ‚Ä¶, family = ¬´ binomial ¬ª) -coeff lu dans estimate lire exp(valeur estimate) a un sens : odd-ratio ÔÉ†pour la regresison multiple on rajoute des var explicatives\nHypoth√®ses : au moins 5 √† 10 √©v√®nements par var explicatives Ex : si une var a expliquer a 54 individus, (age(1), trauma(1) et profession(7)) et 1 + 1 + 7 =9 10(=nbr evenement) = 9054 donc c‚Äôest bon ! Ex : 1 + 1 +7 =95 = 45 54 - passe tout juste pas fou !\nLien recap tests stats https://help.xlstat.com/s/article/guide-de-choix-de-test-statistique?language=fr\nANALYSES DE DONNEES MULTIVARIEES (CF MOOC HUSSON ETC..)\nTableau recap\nRecap de sensi et speci d‚Äôun test :\nEnsuite on d√©finit la sensi comme : Sensi = VP/(VP+FN) c‚Äôest donc la proba que le test accepte H0 si elle est vraie Speci = VN/(VN+FP) c‚Äôest donc la proba que le test accepte H1 si elle est vraie Ensuite on a la VPP et la VPN : VPP : valeur pr√©dite positive = VP/(VP+FP) et c‚Äôest la probabilit√© que H0 soit vraie lorsque le test conclu √† son acceptation VPN : valeur pr√©dite n√©gative = VN/(VN+FN) et c‚Äôest la probabilit√© que H0 soit fausse lorsque le test conclu √† son acceptation\nCourbe ROC : 1-sensi = f(1-speci) = taux de faux positifs = f(taux de vrais positifs) Et FP = 1- sensi FN = 1 ‚Äì recall"
  },
  {
    "objectID": "posts/dev-setup-guide/index.html",
    "href": "posts/dev-setup-guide/index.html",
    "title": "Complete Development Setup Guide for R & Python",
    "section": "",
    "text": "This guide provides a complete and reproducible path to installing R & Python along with a fully configured VS Code setup. It‚Äôs meant to be definitive and fully reproducible."
  },
  {
    "objectID": "posts/dev-setup-guide/index.html#introduction",
    "href": "posts/dev-setup-guide/index.html#introduction",
    "title": "Complete Development Setup Guide for R & Python",
    "section": "",
    "text": "This guide provides a complete and reproducible path to installing R & Python along with a fully configured VS Code setup. It‚Äôs meant to be definitive and fully reproducible."
  },
  {
    "objectID": "posts/dev-setup-guide/index.html#first-you-need-to-install-r",
    "href": "posts/dev-setup-guide/index.html#first-you-need-to-install-r",
    "title": "Complete Development Setup Guide for R & Python",
    "section": "First you need to install R :",
    "text": "First you need to install R :\n\nRecommended way : you can install R via rig which is an installation manager https://github.com/r-lib/rig\n\nWith rig you can very easily install multiple and handle different R versions.\n\nyou can follow the classic R installation via CRAN https://cran.rstudio.com/"
  },
  {
    "objectID": "posts/dev-setup-guide/index.html#then-you-need-to-install-python",
    "href": "posts/dev-setup-guide/index.html#then-you-need-to-install-python",
    "title": "Complete Development Setup Guide for R & Python",
    "section": "Then you need to install python :",
    "text": "Then you need to install python :\n\nfollow classic python installation, python is probably already available if you are with a linux system http://python.org/downloads/\nwe recommend also to install pipx to handle python libvrary installation https://github.com/pypa/pipx"
  },
  {
    "objectID": "posts/dev-setup-guide/index.html#then-we-need-to-install-our-favorite-ide-for-developing",
    "href": "posts/dev-setup-guide/index.html#then-we-need-to-install-our-favorite-ide-for-developing",
    "title": "Complete Development Setup Guide for R & Python",
    "section": "then we need to install our favorite IDE for developing",
    "text": "then we need to install our favorite IDE for developing\nHere we VERY strongly recommend using vscode which allows a complete flexibility for developing in various languages (not only in R) : https://code.visualstudio.com/\nYou can use any other IDE (Rstudio, Pycharm, etc‚Ä¶) but only the configurations of VScode will be developed, feel free to search for your own guides & issues for other IDEs."
  },
  {
    "objectID": "posts/dev-setup-guide/index.html#now-we-can-install-various-tools-for-moden-development",
    "href": "posts/dev-setup-guide/index.html#now-we-can-install-various-tools-for-moden-development",
    "title": "Complete Development Setup Guide for R & Python",
    "section": "Now we can install various tools for moden development",
    "text": "Now we can install various tools for moden development\nIn R, we are using installation with a modern oriented development workflow.\nTo finish installation in vscode to have a complete R integration you need to follow the following steps : - follow R & vscode https://code.visualstudio.com/docs/languages/r\nFor this, we will be having various tools to ensure this workflow :\n\nfirst we‚Äôll be using quarto for authoring, reporting and produce deliverables https://quarto.org/docs/get-started/ and you can install quarto vscode extention\nWe can also install quarto wizard on vscode to healp us easy to use quarto extensions https://github.com/mcanouil/quarto-wizard\n\nTo follow good principles of development, we‚Äôll be using tools to ensure that everything is on track :\nFor formatting/styling code :\n\nair for formatting R code https://posit-dev.github.io/air/editor-vscode.html (alternative to styler)\nruff for formatting python code https://docs.astral.sh/ruff/\nprettier for any other languages https://prettier.io/\n\nFor linting in both R & python you have the errorlens extension that is very complete and useful https://marketplace.visualstudio.com/items/?itemName=usernamehw.errorlens\nFor notebooks in python :\n\nif you want to use notebooks, we strongly recommend using https://github.com/marimo-team/marimo to write in notebooks\notherwise there‚Äôs still the classic jupyter alternatives https://jupyter.org/\n\nThere‚Äôs infinite possibilities for vscode extensions, please install them according to your needs and care for trusted publishers. Here‚Äôs an example command to list your installed extensions:\ncode --list-extensions\nFor reproducible environments and library to manage and handle we have :\n\nuv in python, which is a (COMPLETE AND SUPERIOR) alternative to pyenv and other python pkg management tools https://docs.astral.sh/uv/\nrenv to handle R pkg management which is the most common admitted tool to use in R community BUT there‚Äôs a new tool in town called rv https://github.com/A2-ai/ that is meant to be the uv for R\n\nother resources and setup R/VScode : https://rolkra.github.io/R-VSCode/ https://renkun.me/2022/03/06/my-recommendations-of-vs-code-extensions-for-r/ and more globally various blog posts : https://renkun.me/"
  },
  {
    "objectID": "posts/quarto-getting-started/index.html",
    "href": "posts/quarto-getting-started/index.html",
    "title": "Getting Started with Quarto for Data Science Reports",
    "section": "",
    "text": "Quarto is an open-source scientific and technical publishing system that lets you create:\n\nDocuments (HTML, PDF, Word)\nPresentations\nWebsites and blogs\nBooks\n\nIt supports multiple programming languages including R, Python, Julia, and Observable."
  },
  {
    "objectID": "posts/quarto-getting-started/index.html#what-is-quarto",
    "href": "posts/quarto-getting-started/index.html#what-is-quarto",
    "title": "Getting Started with Quarto for Data Science Reports",
    "section": "",
    "text": "Quarto is an open-source scientific and technical publishing system that lets you create:\n\nDocuments (HTML, PDF, Word)\nPresentations\nWebsites and blogs\nBooks\n\nIt supports multiple programming languages including R, Python, Julia, and Observable."
  },
  {
    "objectID": "posts/quarto-getting-started/index.html#getting-started",
    "href": "posts/quarto-getting-started/index.html#getting-started",
    "title": "Getting Started with Quarto for Data Science Reports",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstallation\nDownload Quarto from quarto.org.\n\n\nYour First Quarto Document\nCreate a file named report.qmd:\n---\ntitle: \"My First Report\"\nauthor: \"Antoine Lucas\"\ndate: today\nformat: html\n---\n\n## Introduction\n\nThis is my first Quarto document.\n\n```{r}\nlibrary(ggplot2)\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Car Weight vs. Fuel Efficiency\")\n```\n\n\nRendering\nquarto render report.qmd"
  },
  {
    "objectID": "posts/quarto-getting-started/index.html#key-features",
    "href": "posts/quarto-getting-started/index.html#key-features",
    "title": "Getting Started with Quarto for Data Science Reports",
    "section": "Key Features",
    "text": "Key Features\n\nCode Execution Options\n#| echo: true      # Show the code\n#| eval: true      # Run the code\n#| warning: false  # Hide warnings\n#| fig-width: 8    # Figure width\n#| fig-height: 6   # Figure height\n\n\nCross-References\nReference figures, tables, and sections:\nSee @fig-scatter for the visualization.\n\n```{r}\n#| label: fig-scatter\n#| fig-cap: \"Scatter plot of weight vs mpg\"\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n```\n\n\nCallouts\n\n\n\n\n\n\nNote\n\n\n\nThis is a note callout for additional information.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is a warning callout for important caveats."
  },
  {
    "objectID": "posts/quarto-getting-started/index.html#benefits-for-data-science",
    "href": "posts/quarto-getting-started/index.html#benefits-for-data-science",
    "title": "Getting Started with Quarto for Data Science Reports",
    "section": "Benefits for Data Science",
    "text": "Benefits for Data Science\n\nSingle source of truth ‚Äî Code and narrative together\nMultiple output formats ‚Äî One document, many formats\nReproducibility ‚Äî Anyone can re-run your analysis\nVersion control friendly ‚Äî Plain text format works well with Git"
  },
  {
    "objectID": "posts/quarto-getting-started/index.html#conclusion",
    "href": "posts/quarto-getting-started/index.html#conclusion",
    "title": "Getting Started with Quarto for Data Science Reports",
    "section": "Conclusion",
    "text": "Conclusion\nQuarto is an excellent tool for creating reproducible data science reports. Start with simple documents and gradually explore its advanced features."
  },
  {
    "objectID": "posts/quarto-getting-started/index.html#resources",
    "href": "posts/quarto-getting-started/index.html#resources",
    "title": "Getting Started with Quarto for Data Science Reports",
    "section": "Resources",
    "text": "Resources\n\nQuarto Documentation\nQuarto Gallery\nAwesome Quarto"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Antoine Lucas",
    "section": "",
    "text": "Email\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Twitter\n  \n  \n    \n     GitHub"
  },
  {
    "objectID": "index.html#what-i-do",
    "href": "index.html#what-i-do",
    "title": "Antoine Lucas",
    "section": "üíº What I Do",
    "text": "üíº What I Do\nI transform complex data challenges into actionable insights. My work spans:\n\nClinical Trial Analytics ‚Äî Statistical analysis and reporting for clinical studies\nFormulation Optimization ‚Äî Data-driven approaches to product development\nQuality Control & Compliance ‚Äî Statistical process control and regulatory analytics\nPredictive Modeling ‚Äî Machine learning for demand forecasting and stability studies\nData Visualization ‚Äî Clear, compelling dashboards and reports for stakeholders"
  },
  {
    "objectID": "index.html#industry-experience",
    "href": "index.html#industry-experience",
    "title": "Antoine Lucas",
    "section": "üî¨ Industry Experience",
    "text": "üî¨ Industry Experience\n\n5+ years working with leading pharma and cosmetic companies on data-driven solutions.\n\nMy experience bridges laboratory data, manufacturing processes, and commercial analytics. I understand the unique challenges of working in regulated industries, including:\n\nGxP compliance requirements\nFDA/EMA regulatory frameworks\nStability testing and shelf-life prediction\nBatch record analysis and deviation investigations"
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "Antoine Lucas",
    "section": "üõ†Ô∏è Technical Skills",
    "text": "üõ†Ô∏è Technical Skills\n\n\n\nDomain\nTechnologies\n\n\n\n\nProgramming\nR, Python, SQL\n\n\nData Analysis\ntidyverse, pandas, scikit-learn\n\n\nVisualization\nggplot2, Plotly, Shiny, Power BI\n\n\nReporting\nQuarto, R Markdown, Jupyter\n\n\nVersion Control\nGit, GitHub\n\n\nDatabases\nPostgreSQL, SQLite, Snowflake\n\n\nCloud\nAWS, Azure basics"
  },
  {
    "objectID": "index.html#core-competencies",
    "href": "index.html#core-competencies",
    "title": "Antoine Lucas",
    "section": "üåü Core Competencies",
    "text": "üåü Core Competencies\n\nStatistical Analysis & Hypothesis Testing\nMachine Learning & Predictive Modeling\nTime Series Analysis\nDesign of Experiments (DoE)\nData Pipeline Development\nTechnical Documentation\nCross-functional Collaboration"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Antoine Lucas",
    "section": "üéì Education",
    "text": "üéì Education\n\nM.Sc. in Data Science / Statistics, 2019 University Name, France\nB.Sc. in Mathematics / Applied Sciences, 2017 University Name, France"
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html",
    "href": "posts/rv-modern-r-package-manager/index.html",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "",
    "text": "I‚Äôve been using renv for years to manage R dependencies, and while it gets the job done, I‚Äôve always felt something was missing. The iterative workflow, the occasional dependency conflicts, the slow installations‚Ä¶ it works, but it doesn‚Äôt feel modern.\nThen I discovered rv ‚Äî a declarative R package manager written in Rust ‚Äî and it immediately clicked. If you‚Äôve been following the Python ecosystem like I have, you‚Äôve probably seen uv revolutionize Python package management. Well, rv brings that same philosophy to R: declarative configuration, fast execution, and true reproducibility. This is exactly what I‚Äôve been waiting for."
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html#introduction",
    "href": "posts/rv-modern-r-package-manager/index.html#introduction",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "",
    "text": "I‚Äôve been using renv for years to manage R dependencies, and while it gets the job done, I‚Äôve always felt something was missing. The iterative workflow, the occasional dependency conflicts, the slow installations‚Ä¶ it works, but it doesn‚Äôt feel modern.\nThen I discovered rv ‚Äî a declarative R package manager written in Rust ‚Äî and it immediately clicked. If you‚Äôve been following the Python ecosystem like I have, you‚Äôve probably seen uv revolutionize Python package management. Well, rv brings that same philosophy to R: declarative configuration, fast execution, and true reproducibility. This is exactly what I‚Äôve been waiting for."
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html#what-is-rv",
    "href": "posts/rv-modern-r-package-manager/index.html#what-is-rv",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "What is rv?",
    "text": "What is rv?\nrv is an R package manager that takes a fundamentally different approach from traditional tools:\n\nDeclarative ‚Äî You describe the desired state of your project, not the steps to get there\nWritten in Rust ‚Äî Compiled for speed (25x faster than alternatives in benchmarks)\nHolistic resolution ‚Äî Resolves the entire dependency tree before installing anything\nLock at install time ‚Äî Captures all installation information when it happens, not retroactively\n\n# Install rv (Linux/macOS)\ncurl -sSL https://raw.githubusercontent.com/A2-ai/rv/refs/heads/main/scripts/install.sh | bash\n\n# Or with Homebrew (macOS)\nbrew tap a2-ai/homebrew-tap\nbrew install rv"
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html#the-problem-with-iterative-installation",
    "href": "posts/rv-modern-r-package-manager/index.html#the-problem-with-iterative-installation",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "The Problem with Iterative Installation",
    "text": "The Problem with Iterative Installation\nIf you‚Äôve used R long enough, you‚Äôve probably experienced this frustration. Traditional R package management (including renv) follows an iterative pattern:\n# The workflow we all know too well\ninstall.packages(\"dplyr\")\ninstall.packages(\"ggplot2\")\n# ... more packages ...\nrenv::snapshot()  # Capture state after the fact\nI‚Äôve run into two issues with this approach more times than I can count:\n\nIncompatible versions ‚Äî Packages installed at different times may have conflicting dependencies. How many times have you had a working project break after updating one package?\nLost context ‚Äî By the time you snapshot, information about how packages were installed is lost. Where did that package come from again?"
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html#rvs-declarative-approach",
    "href": "posts/rv-modern-r-package-manager/index.html#rvs-declarative-approach",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "rv‚Äôs Declarative Approach",
    "text": "rv‚Äôs Declarative Approach\nWith rv, you declare what you want in a rproject.toml file:\n[project]\nname = \"my-analysis\"\nr_version = \"4.5\"\n\nrepositories = [\n    { alias = \"PPM\", url = \"https://packagemanager.posit.co/cran/latest\" },\n]\n\ndependencies = [\n    \"dplyr\",\n    \"ggplot2\",\n    \"tidyr\",\n]\nThen synchronize your project:\nrv sync\nThat‚Äôs it. Seriously. rv resolves the complete dependency tree, ensures compatibility, installs everything, and creates a lockfile ‚Äî all in one atomic operation. The first time I ran this, I was genuinely surprised at how fast it was."
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html#quick-start",
    "href": "posts/rv-modern-r-package-manager/index.html#quick-start",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "Quick Start",
    "text": "Quick Start\n\nInitialize a New Project\nrv init my-project\ncd my-project\nThis creates the following structure:\nmy-project/\n‚îú‚îÄ‚îÄ rv/\n‚îÇ   ‚îú‚îÄ‚îÄ library/\n‚îÇ   ‚îú‚îÄ‚îÄ scripts/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ activate.R\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rvr.R\n‚îÇ   ‚îî‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ .Rprofile\n‚îú‚îÄ‚îÄ rproject.toml\n‚îî‚îÄ‚îÄ rv.lock\n\n\nAdd Dependencies\nEdit rproject.toml or use the CLI:\nrv add dplyr ggplot2 tidyverse\n\n\nCheck What Will Happen\nBefore installing, preview the changes:\nrv plan\n\n\nSynchronize\nrv sync"
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html#key-commands",
    "href": "posts/rv-modern-r-package-manager/index.html#key-commands",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "Key Commands",
    "text": "Key Commands\n\n\n\nCommand\nDescription\n\n\n\n\nrv init\nInitialize a new project\n\n\nrv sync\nSynchronize packages with config\n\n\nrv add &lt;pkg&gt;\nAdd package to config and sync\n\n\nrv plan\nPreview what sync would do\n\n\nrv upgrade\nUpdate packages (ignores lockfile)\n\n\nrv tree\nShow dependency tree\n\n\nrv migrate renv\nMigrate from renv project"
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html#rv-vs-renv-a-comparison",
    "href": "posts/rv-modern-r-package-manager/index.html#rv-vs-renv-a-comparison",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "rv vs renv: A Comparison",
    "text": "rv vs renv: A Comparison\n\n\n\n\n\n\n\n\nFeature\nrenv\nrv\n\n\n\n\nInstallation\nIterative\nDeclarative\n\n\nLock timing\nAfter installation (snapshot)\nAt installation\n\n\nCompatibility check\nNone (trust the process)\nFull tree resolution\n\n\nWritten in\nR\nRust\n\n\nSpeed\nSlower\n~25x faster\n\n\nConfig format\nR code + JSON lockfile\nTOML + lockfile\n\n\n\n\nMigrating from renv\nIf you have an existing renv project:\nrv migrate renv\nThis converts your renv.lock to an rv configuration."
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html#the-uv-parallel-modern-package-management",
    "href": "posts/rv-modern-r-package-manager/index.html#the-uv-parallel-modern-package-management",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "The uv Parallel: Modern Package Management",
    "text": "The uv Parallel: Modern Package Management\nAs someone who works in both R and Python, I find the parallels between rv and Python‚Äôs uv striking ‚Äî and clearly intentional:\n\n\n\nConcept\nPython (uv)\nR (rv)\n\n\n\n\nConfig file\npyproject.toml\nrproject.toml\n\n\nLock file\nuv.lock\nrv.lock\n\n\nSync command\nuv sync\nrv sync\n\n\nAdd package\nuv add\nrv add\n\n\nWritten in\nRust\nRust\n\n\nPhilosophy\nDeclarative, fast\nDeclarative, fast\n\n\n\nBoth tools share the same vision, and honestly, it‚Äôs refreshing to see R catching up:\n\nSpeed through Rust ‚Äî Compiled performance beats interpreted languages\nDeclarative configuration ‚Äî Describe the end state, not the steps\nReproducibility first ‚Äî Lockfiles capture everything needed to recreate the environment\nModern DX ‚Äî Clear CLI, helpful error messages, predictable behavior"
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html#a-modern-r-workflow-rig-rv",
    "href": "posts/rv-modern-r-package-manager/index.html#a-modern-r-workflow-rig-rv",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "A Modern R Workflow: rig + rv",
    "text": "A Modern R Workflow: rig + rv\nHere‚Äôs where things get really exciting for me. Combining rv with rig (the R installation manager) gives you a workflow that finally feels on par with modern Python development:\n\n1. Manage R Versions with rig\n# Install rig (macOS)\nbrew tap r-lib/rig\nbrew install rig\n\n# Or Linux\ncurl -L https://rig.r-lib.org/rig-linux-latest.tar.gz | sudo tar xz -C /usr/local\n\n# List available R versions\nrig available\n\n# Install a specific version\nrig add 4.5\n\n# Set default version\nrig default 4.5\n\n# List installed versions\nrig list\n\n\n2. Manage Packages with rv\n# Initialize project with current R version\nrv init my-analysis\n\n# rv automatically detects R version from rig\ncat rproject.toml\n# [project]\n# r_version = \"4.5\"\n# ...\n\n\n3. The Complete Workflow\n# 1. Ensure you have the right R version\nrig add 4.5\nrig default 4.5\n\n# 2. Create a new project\nrv init data-analysis\ncd data-analysis\n\n# 3. Add your dependencies\nrv add tidyverse arrow DBI\n\n# 4. Start working\nR\n# &gt; library(tidyverse)  # Just works!\n\n\nProject-Specific R Versions\nThe r_version field in rproject.toml ensures teammates use the correct R version:\n[project]\nname = \"critical-analysis\"\nr_version = \"4.4\"  # Pin to R 4.4.x\nWith rig, switching is trivial:\nrig switch 4.4\nrv sync"
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html#advanced-configuration",
    "href": "posts/rv-modern-r-package-manager/index.html#advanced-configuration",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "Advanced Configuration",
    "text": "Advanced Configuration\n\nSpecify Package Sources\ndependencies = [\n    \"dplyr\",\n    { name = \"mypackage\", git = \"https://github.com/org/mypackage.git\", tag = \"v1.0.0\" },\n    { name = \"internal\", repository = \"Internal\" },\n]\n\nrepositories = [\n    { alias = \"PPM\", url = \"https://packagemanager.posit.co/cran/latest\" },\n    { alias = \"Internal\", url = \"https://internal.company.com/r-packages\" },\n]\n\n\nBinary vs Source\nrv intelligently handles binary and source installations, with options to configure per-package:\ndependencies = [\n    { name = \"arrow\", source = true },  # Force source compilation\n]\n\n\nConfigure Compilation\n[project.packages_env_vars.arrow]\nARROW_WITH_S3 = \"ON\"\nARROW_WITH_GCS = \"ON\"\n\n[project.configure_args]\nsf = [\"--with-proj-share=/usr/local/share/proj\"]"
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html#why-this-matters-for-reproducibility",
    "href": "posts/rv-modern-r-package-manager/index.html#why-this-matters-for-reproducibility",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "Why This Matters for Reproducibility",
    "text": "Why This Matters for Reproducibility\nHaving worked in pharma, I know how critical reproducibility is ‚Äî it‚Äôs not just nice to have, it‚Äôs a regulatory requirement. The combination of declarative package management (rv) and explicit R version management (rig) solves the two problems I‚Äôve spent countless hours debugging:\n\n‚ÄúWorks on my machine‚Äù ‚Üí Lockfile captures exact package versions and sources\n‚ÄúWhich R version?‚Äù ‚Üí r_version in config + rig for installation\n\nFor regulated environments (pharma, finance, etc.), this is transformative:\n# Clone a project\ngit clone https://github.com/org/clinical-analysis\ncd clinical-analysis\n\n# Set up exact R version\nrig add $(grep r_version rproject.toml | cut -d'\"' -f2)\n\n# Restore exact package environment\nrv sync\n\n# ‚úÖ Identical environment, guaranteed"
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html#getting-started-today",
    "href": "posts/rv-modern-r-package-manager/index.html#getting-started-today",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "Getting Started Today",
    "text": "Getting Started Today\n\nInstall rig for R version management:\nbrew tap r-lib/rig && brew install rig\nInstall rv for package management:\ncurl -sSL https://raw.githubusercontent.com/A2-ai/rv/refs/heads/main/scripts/install.sh | bash\nCreate your first project:\nrv init my-project\ncd my-project\nrv add tidyverse\nrv sync"
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html#conclusion",
    "href": "posts/rv-modern-r-package-manager/index.html#conclusion",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "Conclusion",
    "text": "Conclusion\nI‚Äôm genuinely excited about rv. It represents a paradigm shift in R package management ‚Äî from iterative, reactive tooling to declarative, proactive workflows. Combined with rig for R version management, it finally gives us a modern development experience on par with what Python developers have enjoyed with uv and pyenv.\nThe R ecosystem is evolving, and I‚Äôm here for it. Tools like rv, rig, and air (the new R formatter) are bringing modern development practices to R. If you value reproducibility, speed, and developer experience like I do, give rv a try. I think you‚Äôll be as impressed as I was."
  },
  {
    "objectID": "posts/rv-modern-r-package-manager/index.html#resources",
    "href": "posts/rv-modern-r-package-manager/index.html#resources",
    "title": "rv: The Modern R Package Manager for Reproducible Workflows",
    "section": "Resources",
    "text": "Resources\n\nrv Documentation\nrv GitHub Repository\nrig - R Installation Manager\nuv - Python Package Manager\nair - R Code Formatter"
  },
  {
    "objectID": "posts/reproducibility-modern-tools/index.html",
    "href": "posts/reproducibility-modern-tools/index.html",
    "title": "The Reproducibility Quest: Modern Tools for R and Python Data Science",
    "section": "",
    "text": "Reproducibility is the cornerstone of credible data science. Whether you‚Äôre in academia, pharma, finance, or tech, the ability to recreate your analysis environment and results is essential. Yet achieving true reproducibility has historically been painful ‚Äî a maze of version conflicts, missing dependencies, and the dreaded ‚Äúworks on my machine‚Äù syndrome.\nThe good news? We‚Äôre living in a golden age of reproducibility tooling. Both R and Python ecosystems have evolved dramatically, offering modern solutions that make reproducible workflows not just possible, but practical. This post explores the complete toolkit for reproducible data science in 2024 and beyond.\nBut before diving into tools, let‚Äôs address a crucial question: how much reproducibility do you actually need?"
  },
  {
    "objectID": "posts/reproducibility-modern-tools/index.html#introduction",
    "href": "posts/reproducibility-modern-tools/index.html#introduction",
    "title": "The Reproducibility Quest: Modern Tools for R and Python Data Science",
    "section": "",
    "text": "Reproducibility is the cornerstone of credible data science. Whether you‚Äôre in academia, pharma, finance, or tech, the ability to recreate your analysis environment and results is essential. Yet achieving true reproducibility has historically been painful ‚Äî a maze of version conflicts, missing dependencies, and the dreaded ‚Äúworks on my machine‚Äù syndrome.\nThe good news? We‚Äôre living in a golden age of reproducibility tooling. Both R and Python ecosystems have evolved dramatically, offering modern solutions that make reproducible workflows not just possible, but practical. This post explores the complete toolkit for reproducible data science in 2024 and beyond.\nBut before diving into tools, let‚Äôs address a crucial question: how much reproducibility do you actually need?"
  },
  {
    "objectID": "posts/reproducibility-modern-tools/index.html#the-reproducibility-spectrum-scoping-your-needs",
    "href": "posts/reproducibility-modern-tools/index.html#the-reproducibility-spectrum-scoping-your-needs",
    "title": "The Reproducibility Quest: Modern Tools for R and Python Data Science",
    "section": "The Reproducibility Spectrum: Scoping Your Needs",
    "text": "The Reproducibility Spectrum: Scoping Your Needs\nNot every project requires the same level of reproducibility investment. A quick exploratory analysis for an internal meeting doesn‚Äôt need the same rigor as a clinical trial submission to the FDA. Understanding where your project falls on this spectrum is essential for efficient resource allocation.\n\nLevels of Reproducibility\n\n\n\n\n\n\n\n\n\nLevel\nDescription\nTypical Use Cases\nInvestment\n\n\n\n\nL1: Minimal\nCode runs on author‚Äôs machine\nPersonal exploration, quick prototypes\nLow\n\n\nL2: Documented\nREADME with manual setup instructions\nTeam projects, handoffs\nLow-Medium\n\n\nL3: Lockfile\nPinned package versions\nProduction analyses, shared projects\nMedium\n\n\nL4: Containerized\nFull environment specification\nRegulated industries, long-term archives\nMedium-High\n\n\nL5: Fully Isolated\nContainer + data versioning + workflow\nClinical trials, financial audits\nHigh\n\n\n\n\n\nScoping Questions\nBefore setting up your reproducibility stack, ask:\n\nWho needs to reproduce this?\n\nJust me, in 6 months? ‚Üí L2-L3\nMy team? ‚Üí L3\nExternal auditors? ‚Üí L4-L5\nThe entire scientific community? ‚Üí L5\n\nWhat‚Äôs the regulatory context?\n\nInternal exploration ‚Üí L1-L2\nPublished research ‚Üí L3-L4\nRegulatory submission ‚Üí L5\n\nWhat‚Äôs the project lifespan?\n\nOne-off analysis ‚Üí L1-L2\nQuarterly reports ‚Üí L3\nMulti-year project ‚Üí L4-L5\n\nWhat‚Äôs the cost of failure?\n\nLearning exercise ‚Üí L1\nBusiness decision ‚Üí L3\nPatient safety ‚Üí L5\n\n\n\n\nThe Reproducibility Tax\nEvery level of reproducibility adds friction:\n\nL1‚ÜíL2: Writing documentation (minutes)\nL2‚ÜíL3: Managing lockfiles, occasional dependency conflicts (hours)\nL3‚ÜíL4: Container setup, debugging build issues (days initially)\nL4‚ÜíL5: Data versioning, workflow management, CI/CD (ongoing effort)\n\nThe goal is to pay the right amount of tax for your context ‚Äî not more, not less."
  },
  {
    "objectID": "posts/reproducibility-modern-tools/index.html#the-convergence-r-and-python-are-aligning",
    "href": "posts/reproducibility-modern-tools/index.html#the-convergence-r-and-python-are-aligning",
    "title": "The Reproducibility Quest: Modern Tools for R and Python Data Science",
    "section": "The Convergence: R and Python Are Aligning",
    "text": "The Convergence: R and Python Are Aligning\nSomething remarkable is happening: the R and Python ecosystems are converging on similar reproducibility paradigms. This isn‚Äôt coincidental ‚Äî it reflects hard-won lessons about what works.\n\nShared Principles\nBoth communities have arrived at the same conclusions:\n\nDeclarative over imperative ‚Äî Define desired state, not installation steps\nLockfiles are essential ‚Äî Capture exact versions at installation time\nTOML for configuration ‚Äî Human-readable, standardized format\nRust for tooling ‚Äî Performance matters for developer experience\nHolistic resolution ‚Äî Solve the entire dependency tree before installing\n\n\n\nThe Modern Stack Comparison\n\n\n\n\n\n\n\n\n\nConcept\nPython\nR\nConvergence\n\n\n\n\nPackage manager\nuv\nrv\nBoth Rust, declarative, TOML-based\n\n\nConfig file\npyproject.toml\nrproject.toml\nSame format, similar structure\n\n\nLockfile\nuv.lock\nrv.lock\nSame purpose, similar approach\n\n\nVersion manager\nuv, pyenv\nrig\nCLI-based, multiple versions\n\n\nFormatter\nruff\nair\nFast, opinionated, Rust-based\n\n\nContainers\ndevcontainers\ndevcontainers + Rocker\nSame specification\n\n\n\nThis convergence means skills transfer between languages, and polyglot projects become easier to manage."
  },
  {
    "objectID": "posts/reproducibility-modern-tools/index.html#package-management-the-foundation",
    "href": "posts/reproducibility-modern-tools/index.html#package-management-the-foundation",
    "title": "The Reproducibility Quest: Modern Tools for R and Python Data Science",
    "section": "Package Management: The Foundation",
    "text": "Package Management: The Foundation\n\nPython: The uv Revolution\nuv has fundamentally changed Python package management. Written in Rust, it‚Äôs blazingly fast and consolidates what used to require multiple tools (pyenv, pip, virtualenv, pip-tools) into one coherent experience.\n# Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create a new project\nuv init my-analysis\ncd my-analysis\n\n# Add dependencies\nuv add pandas numpy scikit-learn matplotlib\n\n# Sync environment\nuv sync\nThe pyproject.toml becomes your single source of truth:\n[project]\nname = \"my-analysis\"\nversion = \"0.1.0\"\nrequires-python = \"&gt;=3.11\"\ndependencies = [\n    \"pandas&gt;=2.0\",\n    \"numpy&gt;=1.24\",\n    \"scikit-learn&gt;=1.3\",\n    \"matplotlib&gt;=3.7\",\n]\n\n[tool.uv]\ndev-dependencies = [\n    \"pytest&gt;=7.0\",\n    \"ruff&gt;=0.1\",\n]\nKey features that make uv essential:\n\nLockfile (uv.lock) ‚Äî Captures exact versions for reproducibility\nPython version management ‚Äî No need for separate pyenv\nBlazing fast ‚Äî 10-100x faster than pip\nDeclarative ‚Äî Describe what you want, not how to get there\n\n# uv can manage Python versions too\nuv python install 3.12\nuv python pin 3.11\n\n# Run scripts with automatic environment setup\nuv run python analysis.py\n\n\nR: The rv Revolution\nOn the R side, rv brings the same declarative philosophy. If you‚Äôve been using renv, rv will feel like a significant upgrade.\n# Install rv\ncurl -sSL https://raw.githubusercontent.com/A2-ai/rv/refs/heads/main/scripts/install.sh | bash\n\n# Create a new project\nrv init my-analysis\ncd my-analysis\n\n# Add dependencies\nrv add tidyverse arrow DBI\n\n# Sync\nrv sync\nYour rproject.toml:\n[project]\nname = \"my-analysis\"\nr_version = \"4.5\"\n\nrepositories = [\n    { alias = \"PPM\", url = \"https://packagemanager.posit.co/cran/latest\" },\n]\n\ndependencies = [\n    \"tidyverse\",\n    \"arrow\",\n    \"DBI\",\n]\n\n\nrenv: The Established Alternative\nWhile rv is the future, renv remains the most widely adopted solution:\n# Initialize renv\nrenv::init()\n\n# Install packages as usual\ninstall.packages(\"tidyverse\")\n\n# Snapshot your dependencies\nrenv::snapshot()\n\n# Restore on another machine\nrenv::restore()\nThe key difference: renv is reactive (snapshot after installation), while rv is declarative (define desired state, then sync).\n\n\nWhen to Use What\n\n\n\n\n\n\n\n\nScenario\nPython\nR\n\n\n\n\nNew projects\nuv\nrv\n\n\nLegacy projects\npip + requirements.txt\nrenv\n\n\nCorporate environments\nuv (or pip if restricted)\nrenv (more established)\n\n\nBleeding edge\nuv\nrv\n\n\nMaximum compatibility\npip\nrenv"
  },
  {
    "objectID": "posts/reproducibility-modern-tools/index.html#language-version-management",
    "href": "posts/reproducibility-modern-tools/index.html#language-version-management",
    "title": "The Reproducibility Quest: Modern Tools for R and Python Data Science",
    "section": "Language Version Management",
    "text": "Language Version Management\n\nR: rig\nrig is the R Installation Manager ‚Äî think of it as pyenv for R:\n# Install rig (macOS)\nbrew tap r-lib/rig && brew install rig\n\n# Install rig (Linux)\ncurl -L https://rig.r-lib.org/rig-linux-latest.tar.gz | sudo tar xz -C /usr/local\n\n# List available versions\nrig available\n\n# Install specific version\nrig add 4.5\nrig add 4.4\n\n# Switch between versions\nrig default 4.5\n\n# List installed versions\nrig list\n\n\nPython: uv or pyenv\nuv now handles Python version management directly:\n# Install Python 3.12\nuv python install 3.12\n\n# Pin project to specific version\nuv python pin 3.11\n\n# List installed versions\nuv python list\nAlternatively, pyenv remains a solid choice:\n# Install pyenv\ncurl https://pyenv.run | bash\n\n# Install Python version\npyenv install 3.11.6\n\n# Set local version for project\npyenv local 3.11.6"
  },
  {
    "objectID": "posts/reproducibility-modern-tools/index.html#environment-control-devcontainers",
    "href": "posts/reproducibility-modern-tools/index.html#environment-control-devcontainers",
    "title": "The Reproducibility Quest: Modern Tools for R and Python Data Science",
    "section": "Environment Control: DevContainers",
    "text": "Environment Control: DevContainers\nPackage managers handle libraries, but what about system dependencies? OpenSSL, GDAL, database drivers, C compilers‚Ä¶ these often cause the most painful reproducibility issues.\n\nWhat Are DevContainers?\nDevelopment Containers provide a complete, containerized development environment. They‚Äôre Docker containers configured specifically for development, with full IDE integration.\nA .devcontainer/devcontainer.json defines your environment:\n{\n    \"name\": \"R & Python Data Science\",\n    \"image\": \"ghcr.io/rocker-org/devcontainer/tidyverse:4.4\",\n    \"features\": {\n        \"ghcr.io/rocker-org/devcontainer-features/quarto-cli:1\": {},\n        \"ghcr.io/devcontainers/features/python:1\": {\n            \"version\": \"3.11\"\n        }\n    },\n    \"customizations\": {\n        \"vscode\": {\n            \"extensions\": [\n                \"REditorSupport.r\",\n                \"ms-python.python\",\n                \"quarto.quarto\"\n            ],\n            \"settings\": {\n                \"r.rterm.linux\": \"/usr/local/bin/R\"\n            }\n        }\n    },\n    \"postCreateCommand\": \"uv sync && rv sync\"\n}\n\n\nWhen to Use DevContainers\nDevContainers are L4 reproducibility ‚Äî use them when:\n\nSystem dependencies are complex (geospatial, databases, specific compilers)\nTeam onboarding needs to be instant\nYou need CI/CD parity with development\nLong-term archival is required\n\nSkip them when:\n\nPure R/Python with no system dependencies\nQuick personal projects\nEnvironments where Docker isn‚Äôt available\n\n\n\nR-Specific DevContainers with Rocker\nThe Rocker Project provides excellent base images:\n{\n    \"name\": \"R Development\",\n    \"image\": \"ghcr.io/rocker-org/devcontainer/tidyverse:4.4\",\n    \"features\": {\n        \"ghcr.io/rocker-org/devcontainer-features/quarto-cli:1\": {},\n        \"ghcr.io/rocker-org/devcontainer-features/rig:1\": {}\n    }\n}\nAvailable images:\n\nrocker/r-ver ‚Äî Base R\nrocker/tidyverse ‚Äî R + tidyverse packages\nrocker/verse ‚Äî tidyverse + LaTeX\nrocker/geospatial ‚Äî Includes GDAL, GEOS, PROJ\n\n\n\nMulti-Language DevContainer\nFor projects using both R and Python:\n{\n    \"name\": \"R + Python Data Science\",\n    \"build\": {\n        \"dockerfile\": \"Dockerfile\"\n    },\n    \"features\": {\n        \"ghcr.io/rocker-org/devcontainer-features/quarto-cli:1\": {}\n    },\n    \"postCreateCommand\": \"rv sync && uv sync\",\n    \"customizations\": {\n        \"vscode\": {\n            \"extensions\": [\n                \"REditorSupport.r\",\n                \"ms-python.python\",\n                \"quarto.quarto\",\n                \"posit.publisher\"\n            ]\n        }\n    }\n}\nWith a custom Dockerfile:\nFROM ghcr.io/rocker-org/devcontainer/tidyverse:4.4\n\n# Install uv for Python\nRUN curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install rv for R\nRUN curl -sSL https://raw.githubusercontent.com/A2-ai/rv/refs/heads/main/scripts/install.sh | bash\n\n# System dependencies\nRUN apt-get update && apt-get install -y \\\n    libpq-dev \\\n    libgdal-dev \\\n    && rm -rf /var/lib/apt/lists/*"
  },
  {
    "objectID": "posts/reproducibility-modern-tools/index.html#workflow-orchestration",
    "href": "posts/reproducibility-modern-tools/index.html#workflow-orchestration",
    "title": "The Reproducibility Quest: Modern Tools for R and Python Data Science",
    "section": "Workflow Orchestration",
    "text": "Workflow Orchestration\nBeyond package management, you need tools to manage the execution flow of your analysis.\n\nR: targets\ntargets is a pipeline tool for R that tracks dependencies and caches results:\n# _targets.R\nlibrary(targets)\n\nlist(\n  tar_target(raw_data, read_csv(\"data/input.csv\")),\n  tar_target(clean_data, clean_data(raw_data)),\n  tar_target(model, fit_model(clean_data)),\n  tar_target(report, render_report(model), format = \"file\")\n)\n# Run the pipeline\nRscript -e \"targets::tar_make()\"\n\n# Visualize dependencies\nRscript -e \"targets::tar_visnetwork()\"\n\n\nPython: Snakemake and DVC\nSnakemake for workflow management:\n# Snakefile\nrule all:\n    input: \"results/final_report.html\"\n\nrule clean_data:\n    input: \"data/raw.csv\"\n    output: \"data/clean.csv\"\n    script: \"scripts/clean.py\"\n\nrule train_model:\n    input: \"data/clean.csv\"\n    output: \"models/model.pkl\"\n    script: \"scripts/train.py\"\nDVC for data versioning:\n# Initialize DVC\ndvc init\n\n# Track large data files\ndvc add data/large_dataset.csv\n\n# Push to remote storage\ndvc push\n\n# Pull data on another machine\ndvc pull"
  },
  {
    "objectID": "posts/reproducibility-modern-tools/index.html#documentation-quarto",
    "href": "posts/reproducibility-modern-tools/index.html#documentation-quarto",
    "title": "The Reproducibility Quest: Modern Tools for R and Python Data Science",
    "section": "Documentation: Quarto",
    "text": "Documentation: Quarto\nQuarto unifies documentation across R, Python, and Julia. A typical Quarto document combines markdown narrative with code chunks in either language.\nKey reproducibility feature: With freeze: auto in your YAML header, Quarto caches computational outputs, so documents can be re-rendered without re-executing all code ‚Äî improving both reproducibility and build times.\nQuarto documents use fenced code blocks with language specifiers like {python} or {r} to indicate executable code, and can output to HTML, PDF, Word, and many other formats."
  },
  {
    "objectID": "posts/reproducibility-modern-tools/index.html#organizational-considerations",
    "href": "posts/reproducibility-modern-tools/index.html#organizational-considerations",
    "title": "The Reproducibility Quest: Modern Tools for R and Python Data Science",
    "section": "Organizational Considerations",
    "text": "Organizational Considerations\n\nTeam Standards\nDifferent teams within an organization may need different reproducibility levels:\n\n\n\nTeam\nTypical Level\nRationale\n\n\n\n\nData Science R&D\nL2-L3\nFast iteration, exploration\n\n\nProduction Analytics\nL3-L4\nReliability, handoffs\n\n\nRegulatory/Compliance\nL4-L5\nAudit requirements\n\n\nExternal Publications\nL4-L5\nPeer review, credibility\n\n\n\n\n\nBuilding a Reproducibility Culture\n\nStart with templates ‚Äî Provide project templates at appropriate levels\nAutomate checks ‚Äî CI/CD that validates reproducibility\nDocument decisions ‚Äî Record why a certain level was chosen\nReview periodically ‚Äî Projects may need to level up over time\n\n\n\nThe Cost-Benefit Reality\nThe key insight: match your investment to your actual needs. Over-engineering reproducibility for a throwaway analysis wastes time. Under-engineering for a regulatory submission creates risk.\nAs project risk and longevity increase, so should your reproducibility investment ‚Äî from L1 (personal exploration) through L5 (regulatory/clinical)."
  },
  {
    "objectID": "posts/reproducibility-modern-tools/index.html#the-complete-reproducibility-checklist",
    "href": "posts/reproducibility-modern-tools/index.html#the-complete-reproducibility-checklist",
    "title": "The Reproducibility Quest: Modern Tools for R and Python Data Science",
    "section": "The Complete Reproducibility Checklist",
    "text": "The Complete Reproducibility Checklist\n\nProject Structure\nmy-project/\n‚îú‚îÄ‚îÄ .devcontainer/\n‚îÇ   ‚îî‚îÄ‚îÄ devcontainer.json     # Container environment (L4+)\n‚îú‚îÄ‚îÄ .github/\n‚îÇ   ‚îî‚îÄ‚îÄ workflows/\n‚îÇ       ‚îî‚îÄ‚îÄ ci.yml            # Automated testing\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # Immutable raw data\n‚îÇ   ‚îî‚îÄ‚îÄ processed/            # Generated data (gitignored)\n‚îú‚îÄ‚îÄ src/                      # Source code\n‚îú‚îÄ‚îÄ notebooks/                # Exploratory work\n‚îú‚îÄ‚îÄ reports/                  # Quarto documents\n‚îú‚îÄ‚îÄ tests/                    # Unit tests\n‚îú‚îÄ‚îÄ _targets.R                # R pipeline (if using targets)\n‚îú‚îÄ‚îÄ pyproject.toml            # Python dependencies\n‚îú‚îÄ‚îÄ uv.lock                   # Python lockfile\n‚îú‚îÄ‚îÄ rproject.toml             # R dependencies\n‚îú‚îÄ‚îÄ rv.lock                   # R lockfile\n‚îú‚îÄ‚îÄ .gitignore\n‚îî‚îÄ‚îÄ README.md\n\n\nBy Level\nL1 (Minimal):\n\nCode in version control\n\nL2 (Documented):\n\nREADME with setup instructions\nList of required packages\n\nL3 (Lockfile):\n\nuv.lock / rv.lock / renv.lock\nLanguage version specified\nCI that runs tests\n\nL4 (Containerized):\n\nDevContainer configuration\nSystem dependencies documented\nCI uses same container\n\nL5 (Fully Isolated):\n\nData versioning (DVC)\nWorkflow management (targets/Snakemake)\nComplete audit trail\nArchived container images"
  },
  {
    "objectID": "posts/reproducibility-modern-tools/index.html#summary-the-modern-reproducibility-toolkit",
    "href": "posts/reproducibility-modern-tools/index.html#summary-the-modern-reproducibility-toolkit",
    "title": "The Reproducibility Quest: Modern Tools for R and Python Data Science",
    "section": "Summary: The Modern Reproducibility Toolkit",
    "text": "Summary: The Modern Reproducibility Toolkit\n\n\n\nNeed\nPython\nR\n\n\n\n\nPackage management\nuv\nrv, renv\n\n\nVersion management\nuv, pyenv\nrig\n\n\nContainerization\ndevcontainers\ndevcontainers + Rocker\n\n\nWorkflow\nSnakemake, DVC\ntargets\n\n\nDocumentation\nQuarto, Jupyter\nQuarto\n\n\nFormatting\nruff\nair\n\n\nLinting\nruff\nlintr"
  },
  {
    "objectID": "posts/reproducibility-modern-tools/index.html#conclusion",
    "href": "posts/reproducibility-modern-tools/index.html#conclusion",
    "title": "The Reproducibility Quest: Modern Tools for R and Python Data Science",
    "section": "Conclusion",
    "text": "Conclusion\nReproducibility is no longer optional ‚Äî it‚Äôs table stakes for professional data science. The tooling has matured to the point where reproducible workflows are not just possible but practical.\nBut remember: reproducibility is a spectrum, not a binary. The right approach depends on your context ‚Äî who needs to reproduce the work, for how long, and at what stakes.\nStart with package management (uv for Python, rv or renv for R), add language version control (rig, uv python), and when system dependencies become complex, embrace devcontainers. Layer in workflow tools like targets or Snakemake for complex pipelines, and document everything with Quarto.\nThe investment in reproducibility pays dividends: faster onboarding, fewer ‚Äúworks on my machine‚Äù incidents, easier debugging, and the confidence that your results can be verified and trusted. Just make sure you‚Äôre investing the right amount for your actual needs."
  },
  {
    "objectID": "posts/reproducibility-modern-tools/index.html#resources",
    "href": "posts/reproducibility-modern-tools/index.html#resources",
    "title": "The Reproducibility Quest: Modern Tools for R and Python Data Science",
    "section": "Resources",
    "text": "Resources\n\nuv Documentation\nrv Documentation\nrenv Documentation\nrig - R Installation Manager\nDevContainers Specification\nRocker Project\ntargets Package\nQuarto\nDVC - Data Version Control\nSnakemake"
  },
  {
    "objectID": "sandbox/project-idea.html",
    "href": "sandbox/project-idea.html",
    "title": "government misconduct-tracker",
    "section": "",
    "text": "Cr√©er un site web avec toutes les ‚Äúcrasses‚Äù du gvt par cat√©gorie/domaine politique ? (Envt, soci√©t√©,etc..) Que ce soit les affaires m√©diatis√©es comme contr√¥le abusif, etc.. + ajouter les promesses non tenues etc.. MAIS TOUT SOURC√â ET LORSQUE AV√âR√â ‚Äì&gt; compilation et bien garder en m√©moire le tout\nS‚Äôamuser a faire une pr√©diction (quand est ce que la prochaine dingz arrive ??)"
  },
  {
    "objectID": "posts/why-curate-resources/index.html",
    "href": "posts/why-curate-resources/index.html",
    "title": "Staying Current in a Fast-Moving Field: Why I Built a Resources Catalog",
    "section": "",
    "text": "If you work in data science, statistics, machine learning, or AI, you know the feeling: you look away for a month, and suddenly there‚Äôs a new paradigm, a new library that makes your favorite tool obsolete, or a paper that fundamentally changes how we think about a problem.\nThe field moves fast. Uncomfortably fast.\n\nStatistics: New causal inference methods, Bayesian workflows, and effect estimation techniques emerge continuously\nMachine Learning: Architecture innovations, training techniques, and interpretability methods evolve monthly\nGenerative AI: The pace since 2022 has been unprecedented ‚Äî LLMs, diffusion models, multimodal systems\nTooling: Package managers, reproducibility tools, and development workflows get better every year\nBest practices: What was considered ‚Äúproduction-ready‚Äù five years ago may now be technical debt\n\nThis creates what I call knowledge debt ‚Äî the gap between what you know and what‚Äôs available. Unlike technical debt, knowledge debt compounds silently. You don‚Äôt realize you‚Äôre behind until you try to solve a problem and discover everyone else is using tools you‚Äôve never heard of."
  },
  {
    "objectID": "posts/why-curate-resources/index.html#the-knowledge-debt-problem",
    "href": "posts/why-curate-resources/index.html#the-knowledge-debt-problem",
    "title": "Staying Current in a Fast-Moving Field: Why I Built a Resources Catalog",
    "section": "",
    "text": "If you work in data science, statistics, machine learning, or AI, you know the feeling: you look away for a month, and suddenly there‚Äôs a new paradigm, a new library that makes your favorite tool obsolete, or a paper that fundamentally changes how we think about a problem.\nThe field moves fast. Uncomfortably fast.\n\nStatistics: New causal inference methods, Bayesian workflows, and effect estimation techniques emerge continuously\nMachine Learning: Architecture innovations, training techniques, and interpretability methods evolve monthly\nGenerative AI: The pace since 2022 has been unprecedented ‚Äî LLMs, diffusion models, multimodal systems\nTooling: Package managers, reproducibility tools, and development workflows get better every year\nBest practices: What was considered ‚Äúproduction-ready‚Äù five years ago may now be technical debt\n\nThis creates what I call knowledge debt ‚Äî the gap between what you know and what‚Äôs available. Unlike technical debt, knowledge debt compounds silently. You don‚Äôt realize you‚Äôre behind until you try to solve a problem and discover everyone else is using tools you‚Äôve never heard of."
  },
  {
    "objectID": "posts/why-curate-resources/index.html#the-bookmark-graveyard",
    "href": "posts/why-curate-resources/index.html#the-bookmark-graveyard",
    "title": "Staying Current in a Fast-Moving Field: Why I Built a Resources Catalog",
    "section": "The Bookmark Graveyard",
    "text": "The Bookmark Graveyard\nLike many practitioners, I used to save interesting links in browser bookmarks. The result? Thousands of unorganized, unsearchable, forgotten URLs. A digital graveyard of good intentions.\nThe problems with bookmarks:\n\nNo context ‚Äî Why did I save this? What problem does it solve?\nNo categorization ‚Äî Is this about Bayesian statistics or Shiny deployment?\nNo language filtering ‚Äî Is this R, Python, or language-agnostic?\nNo discoverability ‚Äî I can‚Äôt browse by topic when exploring\nLink rot ‚Äî URLs die, and I never know until I need them\n\nI needed something better: a structured, searchable, categorized collection that I could actually use."
  },
  {
    "objectID": "posts/why-curate-resources/index.html#building-the-resources-catalog",
    "href": "posts/why-curate-resources/index.html#building-the-resources-catalog",
    "title": "Staying Current in a Fast-Moving Field: Why I Built a Resources Catalog",
    "section": "Building the Resources Catalog",
    "text": "Building the Resources Catalog\nThat‚Äôs why I created the Resources Catalog ‚Äî a curated collection of data science resources organized by type, language, and category.\n\nThe Philosophy\nThe catalog isn‚Äôt trying to be comprehensive. The internet already has ‚Äúawesome lists‚Äù with thousands of links. Instead, it reflects my actual learning journey:\n\nBooks that shaped how I think about problems\nPackages that I‚Äôve found genuinely useful in practice\nBlogs from practitioners who write thoughtfully\nPapers that introduced important concepts\nCourses that are worth the time investment\nTools that improve my daily workflow\n\n\n\nOrganization Principles\nEach resource is tagged with:\n\n\n\n\n\n\n\nField\nPurpose\n\n\n\n\nType\nBook, Blog, Package, Course, Paper, Website, Video, Community\n\n\nLanguage\nR, Python, Other (for language-agnostic resources)\n\n\nCategory\nTopic areas like Statistics, Machine Learning, Bayesian, Shiny, Visualization\n\n\n\nThis structure means I can quickly answer questions like:\n\n‚ÄúWhat are the best resources for learning Bayesian statistics in R?‚Äù\n‚ÄúWhich blogs should I follow for Python machine learning?‚Äù\n‚ÄúWhat packages exist for Shiny production deployment?‚Äù"
  },
  {
    "objectID": "posts/why-curate-resources/index.html#the-domains-that-matter",
    "href": "posts/why-curate-resources/index.html#the-domains-that-matter",
    "title": "Staying Current in a Fast-Moving Field: Why I Built a Resources Catalog",
    "section": "The Domains That Matter",
    "text": "The Domains That Matter\nLooking at my catalog, some patterns emerge. These are the domains where staying current has the highest return on investment:\n\n1. Statistical Methodology\nStatistics isn‚Äôt static. Recent years have seen significant advances in:\n\nCausal inference: Moving beyond correlation to actual causal claims\nBayesian workflows: Stan, brms, and probabilistic programming\nMixed models: Proper handling of hierarchical/longitudinal data\nEffect estimation: Beyond p-values to effect sizes and uncertainty\n\nResources like Statistical Rethinking, the R-Causal community, and blogs from Andrew Gelman help me stay grounded in what statistical practice should look like.\n\n\n2. Machine Learning & Deep Learning\nThe ML landscape shifts constantly:\n\nTransformers everywhere: From NLP to computer vision to time series\nFoundation models: Pre-trained models as starting points\nInterpretability: Understanding what models actually learn\nMLOps: Taking models from notebooks to production\n\nResources like Hugging Face, Fast.ai, and Weights & Biases keep me connected to modern practice.\n\n\n3. Generative AI & LLMs\nThe post-2022 explosion requires active attention:\n\nModel capabilities: What can LLMs actually do reliably?\nPrompt engineering: How to get useful outputs\nIntegration patterns: Using LLMs in applications\nLimitations and risks: What they can‚Äôt do and when they fail\n\nFollowing the Hugging Face blog, research labs like DeepMind, and practitioners on social media helps filter signal from hype.\n\n\n4. Reproducibility & DevOps\nHow we build software keeps improving:\n\nPackage management: uv, renv, rv\nContainerization: Docker, devcontainers\nVersion control: Git workflows, CI/CD\nDocumentation: Quarto, literate programming\n\nThe Turing Way, rOpenSci, and tool-specific documentation keep me current on best practices.\n\n\n5. Visualization & Communication\nData is only useful if you can communicate it:\n\nggplot2 ecosystem: Extensions, themes, customization\nInteractive visualizations: Plotly, htmlwidgets, Shiny\nDashboard design: Layout, UX, performance\nStorytelling: How to structure data narratives\n\nResources like Fundamentals of Data Visualization, the R Graph Gallery, and blogs from visualization practitioners help me communicate better."
  },
  {
    "objectID": "posts/why-curate-resources/index.html#a-learning-system-not-just-a-list",
    "href": "posts/why-curate-resources/index.html#a-learning-system-not-just-a-list",
    "title": "Staying Current in a Fast-Moving Field: Why I Built a Resources Catalog",
    "section": "A Learning System, Not Just a List",
    "text": "A Learning System, Not Just a List\nThe catalog is part of a broader learning system:\n\n1. Capture\nWhen I encounter something interesting ‚Äî a blog post, a package, a paper ‚Äî I add it to the catalog immediately. The friction is low: just add a row to a CSV file.\n\n\n2. Categorize\nTagging forces me to think: What domain is this? What language? What problem does it solve? This metacognition helps retention.\n\n\n3. Review\nPeriodically, I browse the catalog by category. This surfaces forgotten resources and helps me notice patterns (e.g., ‚ÄúI‚Äôve saved a lot about Bayesian methods lately ‚Äî maybe I should actually learn brms properly‚Äù).\n\n\n4. Apply\nThe ultimate test: when facing a problem, can I find a relevant resource quickly? If yes, the system works. If not, I need to improve my categorization or add missing resources.\n\n\n5. Share\nMaking the catalog public creates accountability. It also helps others who are on similar learning journeys."
  },
  {
    "objectID": "posts/why-curate-resources/index.html#the-curators-mindset",
    "href": "posts/why-curate-resources/index.html#the-curators-mindset",
    "title": "Staying Current in a Fast-Moving Field: Why I Built a Resources Catalog",
    "section": "The Curator‚Äôs Mindset",
    "text": "The Curator‚Äôs Mindset\nMaintaining a resource collection requires a specific mindset:\n\nBe Selective\nNot everything deserves a spot. The catalog should answer: ‚ÄúIf I had limited time to learn about X, where would I start?‚Äù Adding everything dilutes value.\n\n\nAccept Impermanence\nLinks die. Tools become obsolete. The catalog needs maintenance. I periodically verify links and remove resources that are no longer relevant.\n\n\nEmbrace Serendipity\nSome of the best discoveries come from browsing adjacent categories. The structured format enables this kind of exploration.\n\n\nBalance Depth and Breadth\nSome domains need deep coverage (statistics, R programming). Others just need a few trusted entry points (specific libraries, niche topics)."
  },
  {
    "objectID": "posts/why-curate-resources/index.html#starting-your-own",
    "href": "posts/why-curate-resources/index.html#starting-your-own",
    "title": "Staying Current in a Fast-Moving Field: Why I Built a Resources Catalog",
    "section": "Starting Your Own",
    "text": "Starting Your Own\nIf you want to build a similar system, start simple:\n\nChoose a format: Spreadsheet, notion, obsidian, or a simple CSV like mine\nDefine your categories: What domains do you care about?\nStart capturing: Add resources as you encounter them\nReview regularly: Browse your collection monthly\nShare if useful: Others might benefit; you‚Äôll be motivated to maintain it\n\nThe key insight: curation is a skill. Like any skill, it improves with practice. Your first version will be imperfect. That‚Äôs fine ‚Äî iterate."
  },
  {
    "objectID": "posts/why-curate-resources/index.html#the-compound-interest-of-learning",
    "href": "posts/why-curate-resources/index.html#the-compound-interest-of-learning",
    "title": "Staying Current in a Fast-Moving Field: Why I Built a Resources Catalog",
    "section": "The Compound Interest of Learning",
    "text": "The Compound Interest of Learning\nStaying current isn‚Äôt about knowing everything. It‚Äôs about:\n\nKnowing where to look when you need something\nRecognizing patterns across domains\nBuilding on foundations rather than starting from scratch each time\nConnecting ideas from different fields\n\nA well-maintained resource collection is infrastructure for this kind of continuous learning. It‚Äôs an investment that compounds over time.\nThe pace of change in data science, ML, and AI isn‚Äôt slowing down. Having a system to navigate that change ‚Äî rather than being overwhelmed by it ‚Äî is increasingly valuable.\nBrowse my Resources Catalog. Fork it, adapt it, build your own. The goal isn‚Äôt to have my collection ‚Äî it‚Äôs to have a collection that serves your learning journey."
  },
  {
    "objectID": "posts/why-curate-resources/index.html#resources-for-resource-curation",
    "href": "posts/why-curate-resources/index.html#resources-for-resource-curation",
    "title": "Staying Current in a Fast-Moving Field: Why I Built a Resources Catalog",
    "section": "Resources for Resource Curation",
    "text": "Resources for Resource Curation\nSome meta-resources on learning and knowledge management:\n\nThe Turing Way ‚Äî Community handbook for reproducible research\nBig Book of R ‚Äî An excellent curated collection of R resources\nAwesome Lists ‚Äî The original curated list concept\nSecond Brain ‚Äî Methodology for personal knowledge management\nData Science Cheatsheet ‚Äî Condensed reference material\n\n\nThe Resources Catalog is continuously updated. Contributions welcome via GitHub."
  },
  {
    "objectID": "posts/livecoding-algorithmic-music/index.html",
    "href": "posts/livecoding-algorithmic-music/index.html",
    "title": "When Code Becomes Music: The Art of Algorithmic Live Performance",
    "section": "",
    "text": "What happens when you treat music not as a sequence of notes, but as a program? When the DJ booth becomes a terminal, and the dance floor responds to algorithms executed in real-time?\nThis is the world of live coding‚Äîa practice where musicians write and modify code during performances, generating music through algorithmic patterns. It‚Äôs a fascinating convergence of computer science, musical composition, and performance art."
  },
  {
    "objectID": "posts/livecoding-algorithmic-music/index.html#the-emergence-of-a-new-artistic-medium",
    "href": "posts/livecoding-algorithmic-music/index.html#the-emergence-of-a-new-artistic-medium",
    "title": "When Code Becomes Music: The Art of Algorithmic Live Performance",
    "section": "",
    "text": "What happens when you treat music not as a sequence of notes, but as a program? When the DJ booth becomes a terminal, and the dance floor responds to algorithms executed in real-time?\nThis is the world of live coding‚Äîa practice where musicians write and modify code during performances, generating music through algorithmic patterns. It‚Äôs a fascinating convergence of computer science, musical composition, and performance art."
  },
  {
    "objectID": "posts/livecoding-algorithmic-music/index.html#strudel-music-in-your-browser",
    "href": "posts/livecoding-algorithmic-music/index.html#strudel-music-in-your-browser",
    "title": "When Code Becomes Music: The Art of Algorithmic Live Performance",
    "section": "Strudel: Music in Your Browser",
    "text": "Strudel: Music in Your Browser\nStrudel is a browser-based live coding environment that brings this artistic practice to anyone with a web browser. Built as a JavaScript port of the legendary Tidal Cycles, it allows you to create complex, evolving musical patterns with surprisingly minimal code.\nHere‚Äôs what a Strudel pattern might look like:\n$: s(\"[bd &lt;hh oh&gt;]*2\").bank(\"tr909\").dec(.4)\nThis single line creates a drum pattern using the TR-909 drum machine sounds‚Äîa bass drum with alternating hi-hats. But the real magic lies in the notation: the &lt;&gt; brackets create alternatives, the *2 doubles the pattern, and suddenly you‚Äôre not just programming, you‚Äôre composing."
  },
  {
    "objectID": "posts/livecoding-algorithmic-music/index.html#why-this-matters-for-data-scientists",
    "href": "posts/livecoding-algorithmic-music/index.html#why-this-matters-for-data-scientists",
    "title": "When Code Becomes Music: The Art of Algorithmic Live Performance",
    "section": "Why This Matters for Data Scientists",
    "text": "Why This Matters for Data Scientists\nAt first glance, live coding music might seem distant from data science. But look closer, and you‚Äôll find striking parallels:\n\n1. Patterns as Data Structures\nIn Strudel, a musical pattern is essentially a time-indexed data structure. Just as we might think about time series data, live coders think about events distributed across cyclical time. The manipulation of these patterns‚Äîfiltering, transforming, combining‚Äîmirrors the operations we perform on datasets.\n\n\n2. Algorithmic Composition as Generative Modeling\nWhen a live coder writes:\nn(\"&lt;0!3 1*2&gt;\").set(chords).mode(\"root:g2\")\n  .voicing().s(\"gm_acoustic_bass\")\nThey‚Äôre essentially defining a generative model‚Äîrules that produce infinite variations within constraints. This is the same thinking behind procedural generation, Monte Carlo simulations, and generative AI.\n\n\n3. Real-Time Feedback Loops\nLive coding is the ultimate REPL experience. You modify code, hear the result immediately, adjust. This tight feedback loop is exactly what makes exploratory data analysis so powerful‚Äîand so satisfying."
  },
  {
    "objectID": "posts/livecoding-algorithmic-music/index.html#the-vision-merging-worlds",
    "href": "posts/livecoding-algorithmic-music/index.html#the-vision-merging-worlds",
    "title": "When Code Becomes Music: The Art of Algorithmic Live Performance",
    "section": "The Vision: Merging Worlds",
    "text": "The Vision: Merging Worlds\nI believe we‚Äôre at the cusp of a new creative paradigm where:\n\nData becomes music: Sonification transforms datasets into auditory experiences, making patterns literally audible. Stock market fluctuations, weather patterns, biological rhythms‚Äîall can become compositional material.\nAlgorithms become instruments: Just as guitars and synthesizers extended human musical capability, algorithmic tools create entirely new sonic possibilities that no human could execute manually.\nDJing becomes programming: The DJ‚Äôs art of reading the room, mixing tracks, and creating flow translates into dynamic, responsive algorithms that can adapt to audience energy, time of day, or any data feed you can imagine.\nPerformance becomes collaboration: Between human and machine, between coder and audience, between prepared material and emergent behavior."
  },
  {
    "objectID": "posts/livecoding-algorithmic-music/index.html#getting-started",
    "href": "posts/livecoding-algorithmic-music/index.html#getting-started",
    "title": "When Code Becomes Music: The Art of Algorithmic Live Performance",
    "section": "Getting Started",
    "text": "Getting Started\nThe beauty of Strudel is its accessibility. No installation required‚Äîjust visit strudel.cc and start experimenting.\nSome entry points:\n\nThe interactive workshop walks you through basics\nThe showcase demonstrates what‚Äôs possible\nThe Discord community welcomes newcomers\n\nFor those with a data science background, pay particular attention to:\n\nSignals: Continuous functions like sine, rand, and perlin noise that modulate parameters\nPattern effects: Transformations like fast(), slow(), chunk(), and rarely() that manipulate temporal structure\nMIDI & OSC: Connect Strudel to other software, hardware, or data sources"
  },
  {
    "objectID": "posts/livecoding-algorithmic-music/index.html#beyond-entertainment",
    "href": "posts/livecoding-algorithmic-music/index.html#beyond-entertainment",
    "title": "When Code Becomes Music: The Art of Algorithmic Live Performance",
    "section": "Beyond Entertainment",
    "text": "Beyond Entertainment\nThis isn‚Äôt just about making cool sounds (though it is that). Live coding represents a different way of thinking about computation‚Äîone that prioritizes expressiveness, exploration, and real-time interaction over optimization and correctness.\nFor data scientists accustomed to notebooks and static analyses, engaging with live coding can:\n\nRefresh your relationship with programming as a creative act\nIntroduce new paradigms for thinking about temporal data\nOpen doors to sonification as an analysis technique\nConnect you with a vibrant open-source community"
  },
  {
    "objectID": "posts/livecoding-algorithmic-music/index.html#the-algorithmic-dj",
    "href": "posts/livecoding-algorithmic-music/index.html#the-algorithmic-dj",
    "title": "When Code Becomes Music: The Art of Algorithmic Live Performance",
    "section": "The Algorithmic DJ",
    "text": "The Algorithmic DJ\nPicture this: a performance where real-time data feeds‚Äîsocial media sentiment, audience movement tracked via sensors, environmental data‚Äîmodulate musical patterns. Where the ‚ÄúDJ‚Äù is both human and algorithm, making decisions together. Where the boundary between instrument and player, between creation and curation, becomes wonderfully blurred.\nThis is not science fiction. Tools like Strudel make it possible today. The question is: what will you create?"
  },
  {
    "objectID": "posts/livecoding-algorithmic-music/index.html#resources",
    "href": "posts/livecoding-algorithmic-music/index.html#resources",
    "title": "When Code Becomes Music: The Art of Algorithmic Live Performance",
    "section": "Resources",
    "text": "Resources\n\nStrudel ‚Äî Browser-based live coding\nTidal Cycles ‚Äî The original Haskell-based pattern language\nAlgorave ‚Äî The movement of algorithmic dance music\nTOPLAP ‚Äî The home of live coding\n\n\nThe intersection of code and creativity is vast and largely unexplored. Whether you‚Äôre a musician curious about algorithms or a programmer curious about music, the door is open."
  },
  {
    "objectID": "sandbox/project-idea.html#catan",
    "href": "sandbox/project-idea.html#catan",
    "title": "government misconduct-tracker",
    "section": "catan",
    "text": "catan\nto see which strategies work best"
  },
  {
    "objectID": "sandbox/project-idea.html#wonders",
    "href": "sandbox/project-idea.html#wonders",
    "title": "government misconduct-tracker",
    "section": "7 wonders",
    "text": "7 wonders\nanalyze card drafting strategies"
  },
  {
    "objectID": "sandbox/project-idea.html#evolution",
    "href": "sandbox/project-idea.html#evolution",
    "title": "government misconduct-tracker",
    "section": "evolution",
    "text": "evolution\nanalyze trait combinations and their effectiveness"
  },
  {
    "objectID": "sandbox/project-idea.html#harmonies",
    "href": "sandbox/project-idea.html#harmonies",
    "title": "government misconduct-tracker",
    "section": "harmonies ?",
    "text": "harmonies ?"
  },
  {
    "objectID": "projects/academic_projects.html",
    "href": "projects/academic_projects.html",
    "title": "Academic Projects",
    "section": "",
    "text": "A collection of projects completed during my final year of engineering school (M2), covering machine learning, data visualization, statistical analysis, and web application development."
  },
  {
    "objectID": "projects/academic_projects.html#machine-learning-statistics",
    "href": "projects/academic_projects.html#machine-learning-statistics",
    "title": "Academic Projects",
    "section": "üß† Machine Learning & Statistics",
    "text": "üß† Machine Learning & Statistics\n\n\n\n\n\n\nEye-Tracking Analysis\n\n\n\nTechnologies: R, Statistical Modeling, Signal Processing\nAnalysis of eye-tracking data to understand visual attention patterns. This project involved processing gaze data, computing fixation metrics, and applying statistical methods to draw insights from visual behavior experiments. We have conducted the project from the protocol development to the data analysis and the report writing with data harvesting using Tobii Pro Lab software and statistical analysis in R.\n View on GitHub\n\n\n\n\n\n\n\n\nMachine Learning Project\n\n\n\n\n\n\n\n\n\nüìÖ M1 Project\n\n\n\nThis project was completed during the first year of Master‚Äôs degree (M1), not during the final year.\n\n\nTechnologies: Python, Scikit-learn, Pandas, NumPy\nA comprehensive machine learning project covering the full ML pipeline: data preprocessing, feature engineering, model selection, hyperparameter tuning, and evaluation. Applied various algorithms including regression, classification, and ensemble methods.\n View on GitHub\n\n\n\n\n\n\n\n\nCo-Inertia Analysis\n\n\n\nTechnologies: R, Multivariate Statistics, ade4\nImplementation and application of co-inertia analysis methods for studying relationships between two data tables. This project explored coupling structures between datasets using factorial methods. Presentation of the project with xaringan html slides.\n View on GitHub"
  },
  {
    "objectID": "projects/academic_projects.html#data-visualization",
    "href": "projects/academic_projects.html#data-visualization",
    "title": "Academic Projects",
    "section": "üìä Data Visualization",
    "text": "üìä Data Visualization\n\n\n\n\n\n\nInteractive Visualization Project\n\n\n\nTechnologies: R, JavaScript, HTML/CSS\nDevelopment of interactive data visualizations to communicate complex information effectively. Focus on user experience, storytelling with data, and creating engaging visual narratives.\n View on GitHub"
  },
  {
    "objectID": "projects/academic_projects.html#web-applications",
    "href": "projects/academic_projects.html#web-applications",
    "title": "Academic Projects",
    "section": "üåê Web Applications",
    "text": "üåê Web Applications\n\n\n\n\n\n\nShiny Web Application (M2)\n\n\n\nTechnologies: R, Shiny, shinydashboard, ggplot2\nDevelopment of an interactive web application using R Shiny for data exploration and visualization. The application provides dynamic filtering, real-time computations, and responsive visualizations.\n View on GitHub"
  },
  {
    "objectID": "projects/academic_projects.html#data-engineering",
    "href": "projects/academic_projects.html#data-engineering",
    "title": "Academic Projects",
    "section": "üîß Data Engineering",
    "text": "üîß Data Engineering\n\n\n\n\n\n\nRedis Database Project\n\n\n\nTechnologies: Redis, Python, NoSQL\nImplementation of a data solution using Redis, an in-memory data structure store. This project covered key-value operations, data persistence, caching strategies, and real-time data handling.\n View on GitHub"
  },
  {
    "objectID": "projects/academic_projects.html#skills-developed",
    "href": "projects/academic_projects.html#skills-developed",
    "title": "Academic Projects",
    "section": "Skills Developed",
    "text": "Skills Developed\n\n\nLanguages\n\nR\nPython\nJavaScript\nSQL\n\n\n\nFrameworks & Tools\n\nShiny / shinydashboard\nScikit-learn\nggplot2\nRmarkdown\n\n\n\nMethods\n\nMachine Learning\nMultivariate Statistics\nData Visualization\nNoSQL Databases"
  },
  {
    "objectID": "sandbox/project-idea.html#for-r-with-quarto-live",
    "href": "sandbox/project-idea.html#for-r-with-quarto-live",
    "title": "government misconduct-tracker",
    "section": "for R with quarto-live",
    "text": "for R with quarto-live\ndata manipulation, visualization with ggplot2, dplyr, etc.. in-depth tutorials on specific topics statistical modeling with lm(), glm(), etc.."
  },
  {
    "objectID": "sandbox/project-idea.html#for-python-with-pyodide",
    "href": "sandbox/project-idea.html#for-python-with-pyodide",
    "title": "government misconduct-tracker",
    "section": "for python with pyodide",
    "text": "for python with pyodide\neasy data manipulation with polars, plotnine, etc.. in-depth tutorials on specific topics statistical modeling with statsmodels, scikit-learn, etc.. machine learning with scikit-learn, tensorflow, etc.."
  }
]